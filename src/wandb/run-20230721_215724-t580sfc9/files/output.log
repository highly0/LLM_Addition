  0%|                                                  | 0/3750 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.








 13%|█████▎                                  | 500/3750 [00:17<01:46, 30.47it/s]
{'loss': 1.9123, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.67}








 27%|██████████▍                            | 1000/3750 [00:37<01:31, 30.05it/s]
  0%|                                                    | 0/63 [00:00<?, ?it/s]
{'loss': 1.7779, 'learning_rate': 1.4666666666666666e-05, 'epoch': 1.33}









 40%|███████████████▍                       | 1482/3750 [00:59<01:14, 30.57it/s]

 40%|███████████████▌                       | 1500/3750 [01:00<01:16, 29.53it/s]








 53%|████████████████████▊                  | 2000/3750 [01:19<00:57, 30.54it/s]
 30%|████████████▋                             | 19/63 [00:00<00:00, 184.62it/s]
{'loss': 1.695, 'learning_rate': 9.333333333333334e-06, 'epoch': 2.67}









 66%|█████████████████████████▊             | 2476/3750 [01:42<00:42, 29.92it/s]
{'loss': 1.6537, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.33}









 80%|███████████████████████████████▏       | 3000/3750 [02:04<00:24, 30.58it/s]
 60%|█████████████████████████▎                | 38/63 [00:00<00:00, 177.80it/s]
{'loss': 1.6343, 'learning_rate': 4.000000000000001e-06, 'epoch': 4.0}










 93%|████████████████████████████████████▍  | 3500/3750 [02:25<00:08, 30.20it/s]
{'loss': 1.6355, 'learning_rate': 1.3333333333333334e-06, 'epoch': 4.67}




100%|███████████████████████████████████████| 3750/3750 [02:38<00:00, 23.72it/s]
100%|██████████████████████████████████████████| 63/63 [00:00<00:00, 175.36it/s]
{'train_runtime': 160.5508, 'train_samples_per_second': 373.713, 'train_steps_per_second': 23.357, 'train_loss': 1.718719873046875, 'epoch': 5.0}
Perplexity: 768.23