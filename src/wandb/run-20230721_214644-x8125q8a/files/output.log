  0%|                                                   | 0/160 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "main.py", line 96, in <module>
    trainer.train()
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 1645, in train
    return inner_training_loop(
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer.py", line 1916, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py", line 677, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/usr/local/lib/python3.8/dist-packages/transformers/trainer_utils.py", line 706, in __call__
    return self.data_collator(features)
  File "/usr/local/lib/python3.8/dist-packages/transformers/data/data_collator.py", line 45, in __call__
    return self.torch_call(features)
  File "/usr/local/lib/python3.8/dist-packages/transformers/data/data_collator.py", line 741, in torch_call
    batch["input_ids"], batch["labels"] = self.torch_mask_tokens(
  File "/usr/local/lib/python3.8/dist-packages/transformers/data/data_collator.py", line 761, in torch_mask_tokens
    special_tokens_mask = [
  File "/usr/local/lib/python3.8/dist-packages/transformers/data/data_collator.py", line 762, in <listcomp>
    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()
  File "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py", line 3552, in get_special_tokens_mask
    special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]
TypeError: 'int' object is not iterable